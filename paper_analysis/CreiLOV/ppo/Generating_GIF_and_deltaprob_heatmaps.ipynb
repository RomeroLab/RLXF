{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6828406f-4773-40df-b5c9-75b641ea3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1bb200-0611-4c0f-a62a-80c4d11dade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define amino acid dictionary for tokenization, define WT for length of context window\n",
    "AAs = 'ACDEFGHIKLMNPQRSTVWY' # setup torchtext vocab to map AAs to indices, usage is aa2ind(list(AAsequence))\n",
    "WT = 'MAGLRHTFVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA' # CreiLOV\n",
    "sequence_length = len(WT)\n",
    "model_identifier ='esm2_t33_650M_UR50D'\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_identifier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f935ecd-c09d-43ec-a1fd-011394e32985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_plot_heatmaps(base_dir, num_updates, model_identifier, output_dir):\n",
    "    # Set up tokens and color map\n",
    "    all_tokens = list(tokenizer.get_vocab().keys())[4:24]\n",
    "    all_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in all_tokens]\n",
    "\n",
    "    # Mutations to annotate: (position, mutant_aa)\n",
    "    mutations_to_annotate = {\n",
    "        4: 'D',   # R5D\n",
    "        6: 'S',   # T7S\n",
    "        25: 'T',  # G26T\n",
    "        111: 'I', # K112I\n",
    "        7: 'I',   # F8I\n",
    "        15: 'E',  # P16E\n",
    "        54: 'L',  # E55L\n",
    "        59: 'K',  # R60K\n",
    "        71: 'I',  # R72I\n",
    "        76: 'K'   # R77K\n",
    "    }\n",
    "    \n",
    "    Magma_r = plt.cm.magma_r(np.linspace(0, 1, 256))\n",
    "    Magma_r[0] = [0, 0, 0, 0.03]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"Modified_Magma_r\", Magma_r, N=256)\n",
    "        \n",
    "    for i in range(num_updates):\n",
    "        model_identifier_name = get_model_identifier(i, model_identifier)\n",
    "        data_path = os.path.join(base_dir, f'update_{i}_probabilities.npy')\n",
    "        probabilities = np.load(data_path)\n",
    "        \n",
    "        plt.figure(figsize=(30, 6))\n",
    "        heatmap = sns.heatmap(probabilities.T, cmap=cmap, square=True, linewidths=0.003, linecolor='0.7', vmin=0, vmax=1)\n",
    "        cbar = heatmap.collections[0].colorbar\n",
    "        cbar.set_label('Predicted Amino Acid Probabilities', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        plt.title(f'Heatmap for {model_identifier_name}')\n",
    "        plt.yticks(np.arange(20) + 0.5, all_tokens, fontsize=8, rotation=0)\n",
    "        plt.xlabel(\"Position in CreiLOV\", fontsize=18)\n",
    "        plt.ylabel('Amino Acid', fontsize=18)\n",
    "\n",
    "        # Add black dots for WT residues and orange dots for mutations\n",
    "        for pos, token in enumerate(WT):  \n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            if token_id in all_token_ids:  # Check if the token exists in the token list\n",
    "                token_index = all_token_ids.index(token_id)\n",
    "                dot_color = 'red' if token != WT[pos] else 'black' # Set dot color based on whether it matches WT or is a mutation\n",
    "                plt.scatter(pos + 0.5, token_index + 0.5, color=dot_color, s=30)  # Adjust dot size as needed\n",
    "\n",
    "        # add annotations with circles for the mutations R5D, T7S, G26T , K112I, F8I, P16E, E55L, R60K, R72I, R77K\n",
    "        for pos, mutant_aa in mutations_to_annotate.items():\n",
    "            token_id = tokenizer.convert_tokens_to_ids(mutant_aa)\n",
    "            if token_id in all_token_ids:  # Check if the token exists in the token list\n",
    "                token_index = all_token_ids.index(token_id)\n",
    "                plt.scatter(pos + 0.5, token_index + 0.5, color='blue', s=30)\n",
    "            else:\n",
    "                print('Issue with token_id ', pos, mutant_aa, token_id)\n",
    "\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=10, label='WT'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Mutation'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Mutations in best sequence variant')\n",
    "        ]\n",
    "                \n",
    "        png_path = os.path.join(output_dir, f'delta_heatmap_update_{i}.png')\n",
    "        svg_path = os.path.join(output_dir, f'delta_heatmap_update_{i}.svg')\n",
    "        plt.savefig(png_path)\n",
    "        plt.savefig(svg_path)\n",
    "        plt.close()\n",
    "    \n",
    "def get_model_identifier(update_index, model_identifier):\n",
    "    if update_index == 0:\n",
    "        return f\"Pre-trained {model_identifier}\"\n",
    "    elif 1 <= update_index <= 11:\n",
    "        return f\"{model_identifier}: SFT Update {update_index}\"\n",
    "    else:\n",
    "        return f\"{model_identifier}: PPO Update {update_index - 11}\"\n",
    "\n",
    "def create_gif(image_dir, output_filename):\n",
    "    images = [Image.open(os.path.join(image_dir, f'heatmap_update_{i}.png')) for i in range(num_updates)]\n",
    "    images[0].save(output_filename, save_all=True, append_images=images[1:], optimize=False, duration=500, loop=3)\n",
    "\n",
    "# # Set directories and number of updates\n",
    "# base_directory = 'SM_probability_data'\n",
    "# output_directory = 'SM_probability_data'\n",
    "# num_updates = 14  # Total updates including pre-trained, SFT, and PPO\n",
    "\n",
    "# # Generate heatmaps and save as PNG\n",
    "# load_and_plot_heatmaps(base_directory, num_updates, model_identifier, output_directory)\n",
    "\n",
    "# # Create GIF from the saved images\n",
    "# create_gif(output_directory, 'SM_probability_data/single_mut_probabilities_across_alignment.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e85d0e-34f4-449c-b7bb-8885b6bbd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_plot_delta_heatmaps(base_dir, num_updates, output_dir, WT):\n",
    "    # Set up tokens and color map\n",
    "    all_tokens = list(tokenizer.get_vocab().keys())[4:24]\n",
    "    all_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in all_tokens]\n",
    "\n",
    "    # Mutations to annotate: (position, mutant_aa)\n",
    "    mutations_to_annotate = {\n",
    "        4: 'D',   # R5D\n",
    "        6: 'S',   # T7S\n",
    "        25: 'T',  # G26T\n",
    "        111: 'I', # K112I\n",
    "        7: 'I',   # F8I\n",
    "        15: 'E',  # P16E\n",
    "        54: 'L',  # E55L\n",
    "        59: 'K',  # R60K\n",
    "        71: 'I',  # R72I\n",
    "        76: 'K'   # R77K\n",
    "    }\n",
    "    \n",
    "    # Load base probabilities from update_0\n",
    "    base_path = os.path.join(base_dir, 'update_0_probabilities.npy')\n",
    "    base_probabilities = np.load(base_path)\n",
    "    \n",
    "    # Define the custom colormap\n",
    "    colors = [(0, '#B2182B'), (0.5, 'white'), (1, '#2166AC')]\n",
    "    cmap_name = 'custom'\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "    \n",
    "    for i in range(0, num_updates):\n",
    "        model_identifier_name = get_model_identifier(i, model_identifier)\n",
    "        current_path = os.path.join(base_dir, f'update_{i}_probabilities.npy')\n",
    "        current_probabilities = np.load(current_path)\n",
    "        delta_probabilities = current_probabilities - base_probabilities\n",
    "        \n",
    "        # Find min and max to set the colormap range and calculate midpoint\n",
    "        min_score = np.min(delta_probabilities)\n",
    "        max_score = np.max(delta_probabilities)\n",
    "        midpoint = abs(min_score) / (max_score - min_score) if (max_score - min_score) != 0 else 0\n",
    "        \n",
    "        plt.figure(figsize=(30, 6))\n",
    "        heatmap = sns.heatmap(delta_probabilities.T, cmap=custom_cmap, square=True, linewidths=0.003, linecolor='0.7', vmin=-1, vmax=1)\n",
    "        cbar = heatmap.collections[0].colorbar\n",
    "        cbar.set_label(f'Predicted Amino Acid Probabilities Relative to ESM2 (650M)', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        plt.title(f'Change in Probabilities for {model_identifier_name}')\n",
    "        plt.yticks(np.arange(20) + 0.5, all_tokens, fontsize=8, rotation=0)\n",
    "        plt.xlabel(\"Position in CreiLOV\", fontsize=18)\n",
    "        plt.ylabel('Amino Acid', fontsize=18)\n",
    "\n",
    "        # Add black dots for WT residues and orange dots for mutations\n",
    "        for pos, token in enumerate(WT):  \n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            if token_id in all_token_ids:  # Check if the token exists in the token list\n",
    "                token_index = all_token_ids.index(token_id)\n",
    "                dot_color = 'red' if token != WT[pos] else 'black' # Set dot color based on whether it matches WT or is a mutation\n",
    "                plt.scatter(pos + 0.5, token_index + 0.5, color=dot_color, s=30)  # Adjust dot size as needed\n",
    "\n",
    "        # add annotations with circles for the mutations R5D, T7S, G26T , K112I, F8I, P16E, E55L, R60K, R72I, R77K\n",
    "        for pos, mutant_aa in mutations_to_annotate.items():\n",
    "            token_id = tokenizer.convert_tokens_to_ids(mutant_aa)\n",
    "            if token_id in all_token_ids:  # Check if the token exists in the token list\n",
    "                token_index = all_token_ids.index(token_id)\n",
    "                plt.scatter(pos + 0.5, token_index + 0.5, color='blue', s=30)\n",
    "            else:\n",
    "                print('Issue with token_id ', pos, mutant_aa, token_id)\n",
    "\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=10, label='WT'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Mutation'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Mutations in best sequence variant')\n",
    "        ]\n",
    "                \n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        png_path = os.path.join(output_dir, f'delta_heatmap_update_{i}.png')\n",
    "        svg_path = os.path.join(output_dir, f'delta_heatmap_update_{i}.svg')\n",
    "        plt.savefig(png_path)\n",
    "        plt.savefig(svg_path)\n",
    "        plt.close()\n",
    "\n",
    "        ### 1-Row Heatmap of Cumulative Non-WT Delta Probabilities ---\n",
    "        cumulative_delta = []\n",
    "        for pos, wt_res in enumerate(WT):\n",
    "            wt_token_id = tokenizer.convert_tokens_to_ids(wt_res)\n",
    "            wt_idx = all_token_ids.index(wt_token_id) if wt_token_id in all_token_ids else None\n",
    "            if wt_idx is None:\n",
    "                cumulative_delta.append(0)\n",
    "                continue\n",
    "\n",
    "            delta_at_pos = delta_probabilities[pos, :]\n",
    "            delta_non_wt = np.sum([v for j, v in enumerate(delta_at_pos) if j != wt_idx])\n",
    "            cumulative_delta.append(delta_non_wt)\n",
    "\n",
    "        plt.figure(figsize=(30, 1.5))\n",
    "        sns.heatmap(np.array(cumulative_delta).reshape(1, -1), cmap=custom_cmap,\n",
    "                    linewidths=0.003, linecolor='0.7', vmin=-1, vmax=1, cbar=True,\n",
    "                    xticklabels=np.arange(1, len(WT) + 1), yticklabels=[\"Δ Non-WT P\"])\n",
    "        plt.title(f'Cumulative Δ Non-WT Probabilities: {model_identifier_name}')\n",
    "        plt.xlabel(\"Position in CreiLOV\", fontsize=18)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        cum_png_path = os.path.join(output_dir, f'cumulative_delta_heatmap_update_{i}.png')\n",
    "        cum_svg_path = os.path.join(output_dir, f'cumulative_delta_heatmap_update_{i}.svg')\n",
    "        plt.savefig(cum_png_path)\n",
    "        plt.savefig(cum_svg_path)\n",
    "        plt.close()\n",
    "\n",
    "def adjust_cmap(cmap, midpoint):\n",
    "    \"\"\" Adjust the midpoint of a colormap. \"\"\"\n",
    "    from matplotlib.colors import DivergingNorm\n",
    "    return cmap if midpoint == 0.5 else LinearSegmentedColormap.from_list(\n",
    "        'Adjusted ' + cmap.name,\n",
    "        cmap(np.linspace(0, 1, 256)),\n",
    "        N=256\n",
    "    )\n",
    "\n",
    "# Set directories and number of updates\n",
    "base_directory = 'SM_probability_data'\n",
    "output_directory = 'SM_probability_data'\n",
    "num_updates = 14  # Total updates including update_0\n",
    "\n",
    "# Generate delta heatmaps and save as PNG\n",
    "load_and_plot_delta_heatmaps(base_directory, num_updates, output_directory, WT)\n",
    "\n",
    "def create_gif(image_dir, output_filename):\n",
    "    images = [Image.open(os.path.join(image_dir, f'delta_heatmap_update_{i}.png')) for i in range(num_updates)]\n",
    "    images[0].save(output_filename, save_all=True, append_images=images[1:], optimize=False, duration=500, loop=3)\n",
    "\n",
    "# Create GIF from the saved images\n",
    "create_gif(output_directory, 'SM_probability_data/delta_single_mut_probabilities_across_alignment.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284473d-0ef7-438d-91e0-3d835eede171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27411e60-fb7b-4780-b634-d319a7ab6a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
