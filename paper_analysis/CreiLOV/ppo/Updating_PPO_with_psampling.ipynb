{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7866bcda-1785-4aac-8a51-133919cf197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from collections import OrderedDict\n",
    "from torchtext import vocab # This package can give problems sometimes, it may be necessary to downgrade to a specific version\n",
    "import seaborn as sns\n",
    "import random\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import itertools\n",
    "import copy\n",
    "import warnings\n",
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168b0d0e-4c5a-45e0-9d12-3194def97ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper scripts\n",
    "from functions import (load_reward_model, identify_mutations_and_count, generate_df, generate_and_evaluate_mutants, mutate_sequences_after_training, mutate_sequences_after_training_esm2_max_sampling)\n",
    "from dataloading_RLXF_ESM2_DDP import (ProtDataModuleESM2, ProtRepDatasetESM2)\n",
    "from PPO_with_psampling_and_model_saving import RLXF_PPO_ESM2\n",
    "from MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e5aab7-b0f7-4559-b61d-db14f077faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define amino acid dictionary for tokenization, define WT for length of context window\n",
    "AAs = 'ACDEFGHIKLMNPQRSTVWY' # setup torchtext vocab to map AAs to indices, usage is aa2ind(list(AAsequence))\n",
    "aa2ind = vocab.vocab(OrderedDict([(a, 1) for a in AAs]))\n",
    "aa2ind.set_default_index(20) # set unknown charcterers to gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127a3e27-740d-4637-a046-43016351f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning esm2_t12_35M_UR50D model from huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:200: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | fixed_model      | EsmForMaskedLM | 34.0 M\n",
      "1 | rl_updated_model | EsmForMaskedLM | 34.0 M\n",
      "----------------------------------------------------\n",
      "34.0 M    Trainable params\n",
      "34.0 M    Non-trainable params\n",
      "68.0 M    Total params\n",
      "271.951   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator: cpu, Number of threads: 8, Strategy: None\n",
      "Loading data to CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d9a5a21a5644cb96a79a0496daec80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "Saved heatmap for single mutant space from WT for sft model\n",
      "Saved heatmap for single mutant space from WT for aligned model\n",
      "Generated sequence with high confidence mutations from fixed model: {43: [('C', 0.95703125)]}\n",
      "Saved heatmap for single mutant space from sequence with high-confidence mutations for sft model\n",
      "Generated sequence with high confidence mutations from aligned model: {43: [('C', 0.95703125)]}\n",
      "Generated ratios for high confidence mutations from aligned model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bx/wt_7f93n19z_0q92_mfgy2y80000gn/T/ipykernel_60398/635758733.py\", line 147, in <module>\n",
      "    trainer.fit(model, dm)\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 579, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 54, in _call_and_handle_interrupt\n",
      "    logger.finalize(\"failed\")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/inspect.py\", line 737, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/inspect.py\", line 744, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1098, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/inspect.py\", line 737, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/nathanielblalock/miniconda3/envs/RLXF/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 60425) is killed by signal: Interrupt: 2. \n"
     ]
    }
   ],
   "source": [
    "################################################## hyperparameters ##################################################\n",
    "# model selections\n",
    "model_identifier ='esm2_t12_35M_UR50D' # esm2_t6_8M_UR50D # esm2_t12_35M_UR50D # esm2_t30_150M_UR50D # esm2_t33_650M_UR50D\n",
    "sft_model = AutoModelForMaskedLM.from_pretrained(f\"facebook/{model_identifier}\")\n",
    "rl_updated_model = AutoModelForMaskedLM.from_pretrained(f\"facebook/{model_identifier}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_identifier}\")\n",
    "num_reward_models = 100 # We have an ensemble of 100 MLP reward models\n",
    "sft_model_path = None # 'SFT_ESM2_650M_with_data_v5_and_random_masking_v0.pt' # path to model to begin PPO (.pt filetype)\n",
    "\n",
    "# learning rate hyperparameters\n",
    "learning_rate = 0.0004\n",
    "lr_mult = 1\n",
    "lr_mult_factor = 1\n",
    "warm_restart = 1 # with warm restart\n",
    "use_scheduler = 1 # with scheduler\n",
    "\n",
    "# optimizer hyperparameters\n",
    "WD = 0.01\n",
    "clip_type = 1 # with gradient clipping\n",
    "grad_clip_threshold = 3.9369403420488362\n",
    "grad_clip_threshold_factor = 1\n",
    "\n",
    "# training hyperparameters\n",
    "seed = 2549\n",
    "batch_size = 1 # Loading WT to dataloader, we generate variant designs each batch so only load WT initially to models\n",
    "epochs = 1000\n",
    "iterations = 1\n",
    "num_updates = int((epochs/100)*iterations) # First restart occurs at 10 epochs (backprop will have occured 10*iterations times)\n",
    "\n",
    "# generating design hyperparameters\n",
    "WT = 'MAGLRHTFVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA' # CreiLOV\n",
    "num_sequences = 2 # 10 # initial batch size\n",
    "inc_batch_size = 3 # increasing batch size each epoch until max_batch_size reached\n",
    "max_batch_size = 2 # 20 # max batch size (dependent on GPU memory)\n",
    "num_mutations = 3 # number of mutations in generated\n",
    "high_conf_threshold = 0.9 # initial probability threshold to be considered high confidence mutation\n",
    "cum_prob_threshold = 0.25 # initial cumulative probability threshold of non-WT resides to be considered candidate position to explore mutating\n",
    "\n",
    "# model dependent hyperparameters\n",
    "num_unfrozen_layers = 27 # initial number of layers of ESM2 unlocked\n",
    "num_layers_unfreeze_each_epoch = 17 # numbers of layers of ESM2 to unlock each epoch until max_num_layers_unfreeze_each_epoch reached\n",
    "max_num_layers_unfreeze_each_epoch = 82 # The max number of layers in ESM2 (650M) that will be aligned cannot exceed 82 -> We can go to at least 71 with bs = 10 on our GPU's @ Duke\n",
    "training_pos_emb = 0 # do not train positional embeddings\n",
    "\n",
    "# important PPO hyperparameters\n",
    "average_type = 2\n",
    "average_type_loss = 0\n",
    "rel_to_WT = 0\n",
    "epsilon = 0.25 # clipping parameter for PPO loss\n",
    "\n",
    "# total reward hyperparameters\n",
    "pairwise_hd_aver_factor = 0.01 # weight for pairwise hamming distance between generated designs each epoch\n",
    "dkl_scale_init = 1e-8 # initial weight for Dkl\n",
    "dkl_scale = 1e-7 # weight term for Dkl after 1st epoch\n",
    "\n",
    "# hyparameters regarding model saving\n",
    "decay = 0.8\n",
    "saving_models_threshold = 10 # do not save models if at 10 # 1.01812135525 # 4.225/4.1498 = generated design fitness / predicted WT fitness\n",
    "filepath = 'toy_Aligning_ESM2_from_SFT_ESM2_with_SA_preference_data_and_psampling_PPO'\n",
    "\n",
    "################################################## hyperparameters ##################################################\n",
    "\n",
    "if sft_model_path is not None:\n",
    "    # Begin PPO with 2 copies of supervised fine-tuned models\n",
    "    state_dict = torch.load(sft_model_path)\n",
    "    sft_model.load_state_dict(state_dict)\n",
    "    rl_updated_model.load_state_dict(state_dict)\n",
    "    for param in sft_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f'Aligning supervised fine-tuned model from {sft_model_path}')\n",
    "else:\n",
    "    # Begin PPO with 2 copies of pretrained models\n",
    "    for param in sft_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(f'Aligning {model_identifier} model from huggingface')\n",
    "\n",
    "# Load models\n",
    "reward_models = []\n",
    "for i in range(num_reward_models):\n",
    "    model_name = f\"best_model_v{i}.ckpt\"\n",
    "    checkpoint_path = f\"./MLP_Reward_Models/{model_name}\"\n",
    "    reward_model = load_reward_model(checkpoint_path)\n",
    "    for param in reward_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    reward_models.append(reward_model)\n",
    "\n",
    "# Determine if we're training on a GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    # Make models reproducible on GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # Set the PYTHONHASHSEED environment variable to the chosen seed to make hash-based operations predictable\n",
    "    np.random.seed(seed) # Set NumPy's random seed to ensure reproducibility of operations using NumPy's random number generator\n",
    "    random.seed(seed) # Set Python's built-in random module's seed to ensure reproducibility of random operations using Python's random functions\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) # Set the seed for generating random numbers in PyTorch to ensure reproducibility on the CPU\n",
    "    torch.cuda.manual_seed(seed) # Set the seed for generating random numbers in PyTorch to ensure reproducibility on the GPU\n",
    "    torch.cuda.manual_seed_all(seed) # Ensure reproducibility for all GPUs by setting the seed for generating random numbers for all CUDA devices\n",
    "    torch.backends.cudnn.deterministic = True # Force cuDNN to use only deterministic convolutional algorithms (can slow down computations but guarantees reproducibility)\n",
    "    torch.backends.cudnn.benchmark = False # Prevent cuDnn from using any algorithms that are nondeterministic\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    accelerator = \"gpu\"\n",
    "    num_devices = torch.cuda.device_count()  # Use all available GPUs\n",
    "    strategy = \"ddp\" if num_devices > 1 else None  # Use DDP if multiple GPUs\n",
    "    print(f\"Accelerator: {accelerator}, Number of devices: {num_devices}, Strategy: {strategy}\")\n",
    "else:\n",
    "    # fix random seeds for reproducibility on CPU\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    accelerator = \"cpu\"\n",
    "    max_threads = 16\n",
    "    num_threads = min(os.cpu_count(), max_threads)  # Use all available CPUs up to a maximum of 16\n",
    "    torch.set_num_threads(num_threads)  # Set the number of threads for PyTorch\n",
    "    num_devices = 1  # Use the CPU\n",
    "    strategy = None\n",
    "    print(f\"Accelerator: {accelerator}, Number of threads: {num_threads}, Strategy: {strategy}\")\n",
    "\n",
    "# Define logger for storing model metrics\n",
    "logger = CSVLogger('logs', name=f\"{filepath}\")\n",
    "version = logger.version\n",
    "\n",
    "# Initialize the RLXF model\n",
    "dm = ProtDataModuleESM2(WT, batch_size, seed)\n",
    "model = RLXF_PPO_ESM2(model_identifier, sft_model, rl_updated_model, reward_models, tokenizer, num_reward_models, sft_model_path, # model selections\n",
    "                num_unfrozen_layers, num_layers_unfreeze_each_epoch, max_num_layers_unfreeze_each_epoch, training_pos_emb, # model dependent hyperparameters\n",
    "                seed, batch_size, epochs, iterations, num_updates, # training hyperparameters\n",
    "                learning_rate, lr_mult, lr_mult_factor, use_scheduler, warm_restart, # learning rate hyperparameters\n",
    "                WD, clip_type, grad_clip_threshold, grad_clip_threshold_factor, # optimizer hyperparameters\n",
    "                WT, num_sequences, inc_batch_size, max_batch_size, num_mutations, high_conf_threshold, cum_prob_threshold, # generating design hyperparameters\n",
    "                average_type_loss, average_type, rel_to_WT, epsilon, # important PPO hyperparameters\n",
    "                pairwise_hd_aver_factor, dkl_scale, dkl_scale_init, # total reward hyperparameters\n",
    "                decay, saving_models_threshold, filepath, version # hyparameters regarding model saving\n",
    "                     )\n",
    "\n",
    "# Trainer setup in PyTorch Lightning\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=epochs,\n",
    "    precision=16 if accelerator == \"gpu\" else 32,  # Mixed precision only on GPU\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    accelerator=accelerator,\n",
    "    num_nodes=1,\n",
    "    devices=num_devices,\n",
    "    strategy=strategy\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423560b-4274-4f20-a133-6a5cfbb0dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model, appending the device name and version number to the filename\n",
    "# model.save_rl_updated_esm2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12675a9-1b96-457f-94f7-7bfd6632f460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406677c-9e9c-4d03-b021-86543cc2d6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
