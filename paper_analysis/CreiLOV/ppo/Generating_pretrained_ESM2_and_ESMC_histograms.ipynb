{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae9dc1c-f1d8-46ac-bb50-a79a8a9c0e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from collections import OrderedDict\n",
    "from torchtext import vocab # This package can give problems sometimes, it may be necessary to downgrade to a specific version\n",
    "import seaborn as sns\n",
    "import random\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pickle\n",
    "from functions import (load_reward_model, identify_mutations_and_count, generate_df, generate_and_evaluate_mutants, generate_and_evaluate_mutants_max_sampling,\n",
    "    mutate_sequences_after_training, mutate_sequences_after_training_esm2_max_sampling, get_sft_version_file)\n",
    "from dataloading_RLXF_ESM2 import (ProtDataModuleESM2, ProtRepDatasetESM2)\n",
    "from PPO_ESM2_650M_with_model_saving_DDP import RLXF_PPO_ESM2\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from MLP import MLP\n",
    "import itertools\n",
    "import copy\n",
    "import warnings\n",
    "import optuna\n",
    "import logging\n",
    "import sys\n",
    "from optuna.exceptions import TrialPruned\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83a507f-a8a6-483d-adca-9f8dce75018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define amino acid dictionary for tokenization, define WT for length of context window\n",
    "AAs = 'ACDEFGHIKLMNPQRSTVWY' # setup torchtext vocab to map AAs to indices, usage is aa2ind(list(AAsequence))\n",
    "WT = 'MAGLRHTFVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA' # CreiLOV\n",
    "aa2ind = vocab.vocab(OrderedDict([(a, 1) for a in AAs]))\n",
    "aa2ind.set_default_index(20) # set unknown charcterers to gap\n",
    "sequence_length = len(WT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580e4cb4-e3a3-4cac-9364-10ecddd332b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602bf950510b45d19496cfb5740c6524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogitsOutput(logits=ForwardTrackData(sequence=tensor([[[-22.0889, -21.9569, -22.0946,  ..., -22.0116, -22.0966, -22.0890],\n",
      "         [-26.7014, -26.6767, -26.6852,  ..., -26.6674, -26.7295, -26.7216],\n",
      "         [-26.7073, -26.7085, -26.7197,  ..., -26.7312, -26.7723, -26.7267],\n",
      "         ...,\n",
      "         [-24.4723, -24.4475, -24.4328,  ..., -24.4337, -24.5339, -24.4628],\n",
      "         [-20.6860, -20.6346, -20.6401,  ..., -20.6410, -20.7196, -20.6419],\n",
      "         [-22.6457, -22.6019, -22.6157,  ..., -22.6052, -22.6929, -22.6211]]]), structure=None, secondary_structure=None, sasa=None, function=None), embeddings=None, residue_annotation_logits=None)\n"
     ]
    }
   ],
   "source": [
    "# Loading ESMC\n",
    "model_identifier ='esmc_600m' # esmc_300m\n",
    "ESMC_model = ESMC.from_pretrained(model_identifier).to(\"cpu\") # or esmc_600m\n",
    "protein = ESMProtein(sequence=WT)\n",
    "protein_tensor = ESMC_model.encode(protein)\n",
    "logits_output = ESMC_model.logits(protein_tensor, LogitsConfig(sequence=True))\n",
    "print(logits_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff980fc-a66c-4e25-a918-60f4ae6a923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__attrs_attrs__', '__attrs_own_setattr__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', 'embeddings', 'logits', 'residue_annotation_logits']\n"
     ]
    }
   ],
   "source": [
    "print(dir(logits_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fc4781-c616-4247-8b3c-3fb9e6c27795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 121, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_output.logits.sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f11ce7-e5c2-41ad-a7b3-2bb22c48613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMProtein(sequence='MAGLRHTFVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESMProtein(sequence=WT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf18bac-b552-40c0-a113-3bb0105b3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_EnsMLPs = 100  # We have 100 reward models\n",
    "num_designs = 1000\n",
    "seed = 7028\n",
    "\n",
    "# Load reward models\n",
    "reward_models = []\n",
    "for i in range(num_EnsMLPs):\n",
    "    model_name = f\"best_model_v{i}.ckpt\"\n",
    "    checkpoint_path = f\"./MLP_Reward_Models/{model_name}\"\n",
    "    reward_model = load_reward_model(checkpoint_path)\n",
    "    for param in reward_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    reward_models.append(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be13be-74cc-4663-b7c3-8833b63da872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a specific position (e.g., position 5)\n",
    "masked_position = 6  # Index of the position to mask (0-based index)\n",
    "masked_sequence = list(WT)  # Convert WT sequence to a mutable list\n",
    "masked_sequence[masked_position] = \"<mask>\"  # Use '*' as a mask token (or appropriate for your model)\n",
    "masked_sequence = ''.join(masked_sequence)  # Convert back to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f208f6-4b27-47f1-9595-2c7aec8c2adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMProtein(sequence='MAGLRH<mask>FVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA', secondary_structure=None, sasa=None, function_annotations=None, coordinates=None, plddt=None, ptm=None, potential_sequence_of_concern=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ESMProtein instance with the masked sequence\n",
    "protein = ESMProtein(sequence=masked_sequence)\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16376ef0-82bb-4315-bd29-2614108a3bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMProteinTensor(sequence=tensor([ 0, 20,  5,  6,  4, 10, 21, 32, 18,  7,  7,  5, 13,  5, 11,  4, 14, 13,\n",
       "        23, 14,  4,  7, 19,  5,  8,  9,  6, 18, 19,  5, 20, 11,  6, 19,  6, 14,\n",
       "        13,  9,  7,  4,  6, 21, 17,  5, 10, 18,  4, 16,  6,  9,  6, 11, 13, 14,\n",
       "        15,  9,  7, 16, 15, 12, 10, 13,  5, 12, 15, 15,  6,  9,  5, 23,  8,  7,\n",
       "        10,  4,  4, 17, 19, 10, 15, 13,  6, 11, 14, 18, 22, 17,  4,  4, 11,  7,\n",
       "        11, 14, 12, 15, 11, 14, 13,  6, 10,  7,  8, 15, 18,  7,  6,  7, 16,  7,\n",
       "        13,  7, 11,  8, 15, 11,  9,  6, 15,  5,  4,  5,  2]), structure=None, secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=None, potential_sequence_of_concern=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the protein sequence\n",
    "protein_tensor = ESMC_model.encode(protein)\n",
    "protein_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d067a6f5-7747-43c4-8278-82c1f59c98a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sequence: MAGLRH<mask>FVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA\n",
      "Probabilities for masked position: tensor([4.4926e-21, 4.7351e-21, 4.6001e-21, 3.1521e-07, 9.0963e-04, 4.9500e-02,\n",
      "        3.4532e-02, 4.2323e-03, 5.8087e-01, 4.6608e-03, 2.9080e-03, 7.5115e-02,\n",
      "        9.6038e-04, 1.9427e-02, 7.8629e-03, 1.2239e-03, 4.8081e-03, 1.1435e-01,\n",
      "        1.8392e-03, 3.4115e-03, 1.6124e-03, 1.9138e-02, 1.2737e-03, 7.1365e-02,\n",
      "        7.6950e-07, 2.3488e-11, 2.9986e-08, 1.0042e-12, 2.9761e-13, 4.1920e-21,\n",
      "        4.4152e-21, 4.8907e-21, 4.5065e-21, 4.6019e-21, 4.5237e-21, 4.9978e-21,\n",
      "        4.7506e-21, 5.0350e-21, 4.9012e-21, 4.8875e-21, 4.7245e-21, 4.7894e-21,\n",
      "        4.3468e-21, 4.8849e-21, 4.6155e-21, 4.6724e-21, 4.7003e-21, 4.6213e-21,\n",
      "        4.7450e-21, 4.6686e-21, 4.6493e-21, 4.4516e-21, 5.9318e-21, 4.7724e-21,\n",
      "        4.8365e-21, 4.6127e-21, 4.8750e-21, 5.0812e-21, 5.0008e-21, 4.8811e-21,\n",
      "        4.3710e-21, 4.9991e-21, 4.3486e-21, 4.7678e-21])\n"
     ]
    }
   ],
   "source": [
    "# Get logits\n",
    "logits_output = ESMC_model.logits(protein_tensor, LogitsConfig(sequence=True))\n",
    "\n",
    "# Index logits for the masked position\n",
    "sequence_logits = logits_output.logits.sequence.squeeze(0)  # Remove batch dimension\n",
    "masked_position_logits = sequence_logits[masked_position+1]  # Get logits for the masked position\n",
    "probs = torch.nn.functional.softmax(masked_position_logits, dim=-1)\n",
    "\n",
    "# Output results\n",
    "print(\"Masked Sequence:\", masked_sequence)\n",
    "print(\"Probabilities for masked position:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d0324b-b205-40bc-a79c-248801b77ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Token <cls>\n",
      "Index 1: Token <pad>\n",
      "Index 2: Token <eos>\n",
      "Index 3: Token <unk>\n",
      "Index 4: Token L\n",
      "Index 5: Token A\n",
      "Index 6: Token G\n",
      "Index 7: Token V\n",
      "Index 8: Token S\n",
      "Index 9: Token E\n",
      "Index 10: Token R\n",
      "Index 11: Token T\n",
      "Index 12: Token I\n",
      "Index 13: Token D\n",
      "Index 14: Token P\n",
      "Index 15: Token K\n",
      "Index 16: Token Q\n",
      "Index 17: Token N\n",
      "Index 18: Token F\n",
      "Index 19: Token Y\n",
      "Index 20: Token M\n",
      "Index 21: Token H\n",
      "Index 22: Token W\n",
      "Index 23: Token C\n",
      "Index 24: Token X\n",
      "Index 25: Token B\n",
      "Index 26: Token U\n",
      "Index 27: Token Z\n",
      "Index 28: Token O\n",
      "Index 29: Token .\n",
      "Index 30: Token -\n",
      "Index 31: Token |\n",
      "Index 32: Token <mask>\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_VOCAB = [\n",
    "    \"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\",\n",
    "    \"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\",\n",
    "    \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\", \"X\", \"B\", \"U\", \"Z\",\n",
    "    \"O\", \".\", \"-\", \"|\",\n",
    "    \"<mask>\",\n",
    "]\n",
    "\n",
    "# Convert SEQUENCE_VOCAB to a dictionary\n",
    "token_dict = {index: token for index, token in enumerate(SEQUENCE_VOCAB)}\n",
    "\n",
    "# Print the dictionary\n",
    "for index, token in token_dict.items():\n",
    "    print(f\"Index {index}: Token {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c776465f-0e43-420c-966c-8d6dc308acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.sdk.forge import ESM3ForgeInferenceClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67027dc-14e4-4a44-90ca-5859c08a62fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sequence: MAGLRH<mask>FVVADATLPDCPLVYASEGFYAMTGYGPDEVLGHNARFLQGEGTDPKEVQKIRDAIKKGEACSVRLLNYRKDGTPFWNLLTVTPIKTPDGRVSKFVGVQVDVTSKTEGKALA\n",
      "Probabilities for masked position: tensor([9.8710e-16, 9.8710e-16, 9.8710e-16, 5.0826e-01, 1.7666e-02, 1.1676e-02,\n",
      "        1.3545e-02, 1.0467e-02, 1.3335e-02, 2.5108e-02, 4.8776e-02, 5.9404e-03,\n",
      "        4.0195e-03, 7.3354e-03, 5.7802e-03, 6.3235e-03, 1.2237e-02, 3.2806e-03,\n",
      "        5.7017e-03, 3.3290e-03, 1.4055e-03, 7.4219e-03, 4.6627e-03, 1.1676e-02,\n",
      "        2.7205e-01, 3.4011e-10, 1.5750e-12, 1.6066e-10, 3.1014e-13, 9.8710e-16,\n",
      "        9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16,\n",
      "        9.8710e-16, 8.7111e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16,\n",
      "        8.7111e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16,\n",
      "        9.8710e-16, 9.8710e-16, 8.7111e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16,\n",
      "        9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16,\n",
      "        9.8710e-16, 9.8710e-16, 9.8710e-16, 9.8710e-16])\n"
     ]
    }
   ],
   "source": [
    "ESMC_model = ESM3ForgeInferenceClient(model=\"esmc-6b-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"7cebzsdq955rf3p2LlRHsz\")\n",
    "protein_tensor = ESMC_model.encode(protein)\n",
    "\n",
    "# Get logits\n",
    "logits_output = ESMC_model.logits(protein_tensor, LogitsConfig(sequence=True))\n",
    "\n",
    "# Index logits for the masked position\n",
    "sequence_logits = logits_output.logits.sequence.squeeze(0)  # Remove batch dimension\n",
    "masked_position_logits = sequence_logits[masked_position+1]  # Get logits for the masked position\n",
    "probs = torch.nn.functional.softmax(masked_position_logits, dim=-1)\n",
    "\n",
    "# Output results\n",
    "print(\"Masked Sequence:\", masked_sequence)\n",
    "print(\"Probabilities for masked position:\", probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
